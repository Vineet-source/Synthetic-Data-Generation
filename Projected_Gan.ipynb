{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM8DcqPJYZQFt5NhWOqbEVQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vineet-source/Synthetic-Data-Generation/blob/main/Projected_Gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAswbXUAUHwQ",
        "outputId": "46b6df87-034b-458f-d443-89ab22f22c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Dataset URL: https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading skin-cancer-mnist-ham10000.zip to /content\n",
            "100% 5.19G/5.20G [00:11<00:00, 158MB/s]\n",
            "100% 5.20G/5.20G [00:12<00:00, 460MB/s]\n",
            "Dataset downloaded and extracted.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Kaggle API\n",
        "!pip install kaggle\n",
        "\n",
        "# Step 2: Create the Kaggle directory\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Step 3: Upload your kaggle.json API key file using the Colab file browser\n",
        "# (In the Colab sidebar, click the folder icon, then the upload icon, and select your kaggle.json)\n",
        "# After uploading, run:\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Step 4: Download the HAM10000 dataset from Kaggle\n",
        "!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000 -p /content/\n",
        "\n",
        "# Step 5: Unzip the dataset\n",
        "!unzip -q /content/skin-cancer-mnist-ham10000.zip -d /content/skin_cancer_dataset\n",
        "\n",
        "print(\"Dataset downloaded and extracted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "metadata_csv = '/content/skin_cancer_dataset/HAM10000_metadata.csv'\n",
        "images_dir_1 = '/content/skin_cancer_dataset/HAM10000_images_part_1'\n",
        "images_dir_2 = '/content/skin_cancer_dataset/HAM10000_images_part_2'  # If this exists\n",
        "output_dir = 'organized_skin_lesions'\n",
        "\n",
        "# Read metadata\n",
        "df = pd.read_csv(metadata_csv)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Helper: Check which part the image is in\n",
        "def get_image_path(image_id):\n",
        "    fname = image_id + '.jpg'\n",
        "    path1 = os.path.join(images_dir_1, fname)\n",
        "    if os.path.exists(path1):\n",
        "        return path1\n",
        "    path2 = os.path.join(images_dir_2, fname)\n",
        "    if os.path.exists(path2):\n",
        "        return path2\n",
        "    return None  # Image not found\n",
        "\n",
        "# Organize images into class subfolders\n",
        "not_found = []\n",
        "for idx, row in df.iterrows():\n",
        "    image_id = row['image_id']\n",
        "    class_label = row['dx']\n",
        "    src_img = get_image_path(image_id)\n",
        "    if src_img is None:\n",
        "        not_found.append(image_id)\n",
        "        continue\n",
        "    class_dir = os.path.join(output_dir, class_label)\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    dst_img = os.path.join(class_dir, image_id + '.jpg')\n",
        "    shutil.copy(src_img, dst_img)\n",
        "\n",
        "print(\"Skin cancer dataset organized into class subfolders!\")\n",
        "if not_found:\n",
        "    print(f\"Warning: {len(not_found)} images not found.\")\n",
        "\n",
        "# Optional: Print example structure\n",
        "def print_directory_tree(path, level=2):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        depth = root[len(path):].count(os.sep)\n",
        "        if depth > level:\n",
        "            continue\n",
        "        indent = ' ' * 4 * depth\n",
        "        print(f'{indent}{os.path.basename(root)}/')\n",
        "        if depth < level:\n",
        "            for f in files[:5]:  # Print only first 5 files per folder for brevity\n",
        "                print(f'{indent}    {f}')\n",
        "\n",
        "print_directory_tree(output_dir, level=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDc65UOpVvkj",
        "outputId": "6aefe59c-0c47-4f0a-976d-6ea174265ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skin cancer dataset organized into class subfolders!\n",
            "organized_skin_lesions/\n",
            "    vasc/\n",
            "        ISIC_0025452.jpg\n",
            "        ISIC_0032557.jpg\n",
            "        ISIC_0025606.jpg\n",
            "        ISIC_0031197.jpg\n",
            "        ISIC_0032057.jpg\n",
            "    akiec/\n",
            "        ISIC_0026083.jpg\n",
            "        ISIC_0024575.jpg\n",
            "        ISIC_0026650.jpg\n",
            "        ISIC_0031672.jpg\n",
            "        ISIC_0030491.jpg\n",
            "    nv/\n",
            "        ISIC_0026841.jpg\n",
            "        ISIC_0030886.jpg\n",
            "        ISIC_0026550.jpg\n",
            "        ISIC_0024606.jpg\n",
            "        ISIC_0025962.jpg\n",
            "    df/\n",
            "        ISIC_0033860.jpg\n",
            "        ISIC_0027598.jpg\n",
            "        ISIC_0027008.jpg\n",
            "        ISIC_0027727.jpg\n",
            "        ISIC_0031429.jpg\n",
            "    bcc/\n",
            "        ISIC_0029083.jpg\n",
            "        ISIC_0024573.jpg\n",
            "        ISIC_0026687.jpg\n",
            "        ISIC_0028577.jpg\n",
            "        ISIC_0026528.jpg\n",
            "    mel/\n",
            "        ISIC_0032622.jpg\n",
            "        ISIC_0024961.jpg\n",
            "        ISIC_0033279.jpg\n",
            "        ISIC_0031529.jpg\n",
            "        ISIC_0027163.jpg\n",
            "    bkl/\n",
            "        ISIC_0026070.jpg\n",
            "        ISIC_0027780.jpg\n",
            "        ISIC_0026016.jpg\n",
            "        ISIC_0025856.jpg\n",
            "        ISIC_0029522.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset_preprocessed.py\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "def load_data(data_path, batch_size):\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    train_data = datasets.ImageFolder(data_path, transform=train_transforms)\n",
        "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    return train_data, trainloader\n",
        "\n",
        "\n",
        "def save_transformed_images(dataset, save_dir, num_images=100):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    for idx, (image_tensor, label) in enumerate(dataset):\n",
        "        if idx >= num_images:\n",
        "            break\n",
        "        # Denormalize before saving\n",
        "        image_tensor = image_tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "        image_tensor = image_tensor + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        image_tensor = torch.clamp(image_tensor, 0, 1)  # Clamp to valid range\n",
        "        save_path = os.path.join(save_dir, f\"img_{idx}_label_{label}.png\")\n",
        "        save_image(image_tensor, save_path)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dataset, loader = load_data(\"organized_skin_lesions\", 32)\n",
        "    save_transformed_images(dataset, \"resized_images\", num_images=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQHV_DozU5ll",
        "outputId": "a134610c-d764-4fcb-9dc5-8ec61f67a121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset_preprocessed.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.py\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "\n",
        "def load_data(data_path, batch_size):\n",
        "    train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                           transforms.Resize((256, 256)),\n",
        "                                           transforms.RandomHorizontalFlip(),\n",
        "                                           transforms.ToTensor(),\n",
        "                                           transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                                [0.229, 0.224, 0.225])])\n",
        "    train_data = datasets.ImageFolder(data_path, transform=train_transforms)\n",
        "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    return trainloader\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    loader= load_data(\"organized_skin_lesions\",32)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2cI6_aAVvVV",
        "outputId": "9abcf10b-29f9-4f4c-c70b-aa24e4d60ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile differentiable_augmentation.py\n",
        "\"\"\"\n",
        "Differentiable Augmentation for Data-Efficient GAN Training\n",
        "Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "https://arxiv.org/pdf/2006.10738\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# def diffaug(x, policy='', channels_first=True):\n",
        "#     if policy:\n",
        "#         if not channels_first:\n",
        "#             x = x.permute(0, 3, 1, 2)\n",
        "#         for p in policy.split(','):\n",
        "#             for f in AUGMENT_FNS[p]:\n",
        "#                 x = f(x)\n",
        "#         if not channels_first:\n",
        "#             x = x.permute(0, 2, 3, 1)\n",
        "#         x = x.contiguous()\n",
        "#     return x\n",
        "\n",
        "\n",
        "class DiffAugment:\n",
        "    def __init__(self, policy='', channels_first=True):\n",
        "        self.policies = policy.split(',')\n",
        "        self.channels_first = channels_first\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.policies:\n",
        "            if not self.channels_first:\n",
        "                x = x.permute(0, 3, 1, 2)\n",
        "            for p in self.policies:\n",
        "                for f in AUGMENT_FNS[p]:\n",
        "                    x = f(x)\n",
        "            if not self.channels_first:\n",
        "                x = x.permute(0, 2, 3, 1)\n",
        "            x = x.contiguous()\n",
        "        return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2).contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxwzXSwpVvG0",
        "outputId": "2955c88b-0d7b-4e86-8110-6b908d3e9988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing differentiable_augmentation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile efficient_net.py\n",
        "\"\"\"Code taken and slightly modified from https://github.com/RangiLyu/EfficientNet-Lite\"\"\"\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "efficientnet_lite_params = {\n",
        "    # width_coefficient, depth_coefficient, image_size, dropout_rate\n",
        "    'efficientnet_lite0': [1.0, 1.0, 224, 0.2],\n",
        "    'efficientnet_lite1': [1.0, 1.1, 240, 0.2],\n",
        "    'efficientnet_lite2': [1.1, 1.2, 260, 0.3],\n",
        "    'efficientnet_lite3': [1.2, 1.4, 280, 0.3],\n",
        "    'efficientnet_lite4': [1.4, 1.8, 300, 0.3],\n",
        "}\n",
        "\n",
        "\n",
        "def round_filters(filters, multiplier, divisor=8, min_width=None):\n",
        "    \"\"\"Calculate and round number of filters based on width multiplier.\"\"\"\n",
        "    if not multiplier:\n",
        "        return filters\n",
        "    filters *= multiplier\n",
        "    min_width = min_width or divisor\n",
        "    new_filters = max(min_width, int(filters + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_filters < 0.9 * filters:\n",
        "        new_filters += divisor\n",
        "    return int(new_filters)\n",
        "\n",
        "def round_repeats(repeats, multiplier):\n",
        "    \"\"\"Round number of filters based on depth multiplier.\"\"\"\n",
        "    if not multiplier:\n",
        "        return repeats\n",
        "    return int(math.ceil(multiplier * repeats))\n",
        "\n",
        "def drop_connect(x, drop_connect_rate, training):\n",
        "    if not training:\n",
        "        return x\n",
        "    keep_prob = 1.0 - drop_connect_rate\n",
        "    batch_size = x.shape[0]\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=x.dtype, device=x.device)\n",
        "    binary_mask = torch.floor(random_tensor)\n",
        "    x = (x / keep_prob) * binary_mask\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class MBConvBlock(nn.Module):\n",
        "    def __init__(self, inp, final_oup, k, s, expand_ratio, se_ratio, has_se=False):\n",
        "        super(MBConvBlock, self).__init__()\n",
        "\n",
        "        self._momentum = 0.01\n",
        "        self._epsilon = 1e-3\n",
        "        self.input_filters = inp\n",
        "        self.output_filters = final_oup\n",
        "        self.stride = s\n",
        "        self.expand_ratio = expand_ratio\n",
        "        self.has_se = has_se\n",
        "        self.id_skip = True  # skip connection and drop connect\n",
        "\n",
        "        # Expansion phase\n",
        "        oup = inp * expand_ratio  # number of output channels\n",
        "        if expand_ratio != 1:\n",
        "            self._expand_conv = nn.Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
        "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._momentum, eps=self._epsilon)\n",
        "\n",
        "        # Depthwise convolution phase\n",
        "        self._depthwise_conv = nn.Conv2d(\n",
        "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
        "            kernel_size=k, padding=(k - 1) // 2, stride=s, bias=False)\n",
        "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._momentum, eps=self._epsilon)\n",
        "\n",
        "        # Squeeze and Excitation layer, if desired\n",
        "        if self.has_se:\n",
        "            num_squeezed_channels = max(1, int(inp * se_ratio))\n",
        "            self._se_reduce = nn.Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
        "            self._se_expand = nn.Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
        "\n",
        "        # Output phase\n",
        "        self._project_conv = nn.Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
        "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._momentum, eps=self._epsilon)\n",
        "        self._relu = nn.ReLU6(inplace=True)\n",
        "\n",
        "    def forward(self, x, drop_connect_rate=None):\n",
        "        \"\"\"\n",
        "        :param x: input tensor\n",
        "        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n",
        "        :return: output of block\n",
        "        \"\"\"\n",
        "\n",
        "        # Expansion and Depthwise Convolution\n",
        "        identity = x\n",
        "        if self.expand_ratio != 1:\n",
        "            x = self._relu(self._bn0(self._expand_conv(x)))\n",
        "        x = self._relu(self._bn1(self._depthwise_conv(x)))\n",
        "\n",
        "        # Squeeze and Excitation\n",
        "        if self.has_se:\n",
        "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
        "            x_squeezed = self._se_expand(self._relu(self._se_reduce(x_squeezed)))\n",
        "            x = torch.sigmoid(x_squeezed) * x\n",
        "\n",
        "        x = self._bn2(self._project_conv(x))\n",
        "\n",
        "        # Skip connection and drop connect\n",
        "        if self.id_skip and self.stride == 1  and self.input_filters == self.output_filters:\n",
        "            if drop_connect_rate:\n",
        "                x = drop_connect(x, drop_connect_rate, training=self.training)\n",
        "            x += identity  # skip connection\n",
        "        return x\n",
        "\n",
        "\n",
        "class EfficientNetLite(nn.Module):\n",
        "    def __init__(self, widthi_multiplier, depth_multiplier, num_classes, drop_connect_rate, dropout_rate):\n",
        "        super(EfficientNetLite, self).__init__()\n",
        "\n",
        "        # Batch norm parameters\n",
        "        momentum = 0.01\n",
        "        epsilon = 1e-3\n",
        "        self.drop_connect_rate = drop_connect_rate\n",
        "\n",
        "        mb_block_settings = [\n",
        "            #repeat|kernal_size|stride|expand|input|output|se_ratio\n",
        "            [1, 3, 1, 1, 32,  16,  0.25],\n",
        "            [2, 3, 2, 6, 16,  24,  0.25],\n",
        "            [2, 5, 2, 6, 24,  40,  0.25],\n",
        "            [3, 3, 2, 6, 40,  80,  0.25],\n",
        "            [3, 5, 1, 6, 80,  112, 0.25],\n",
        "            [4, 5, 2, 6, 112, 192, 0.25],\n",
        "            [1, 3, 1, 6, 192, 320, 0.25]\n",
        "        ]\n",
        "\n",
        "        # Stem\n",
        "        out_channels = 32\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, out_channels, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=out_channels, momentum=momentum, eps=epsilon),\n",
        "            nn.ReLU6(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Build blocks\n",
        "        self.blocks = nn.ModuleList([])\n",
        "        for i, stage_setting in enumerate(mb_block_settings):\n",
        "            stage = nn.ModuleList([])\n",
        "            num_repeat, kernal_size, stride, expand_ratio, input_filters, output_filters, se_ratio = stage_setting\n",
        "            # Update block input and output filters based on width multiplier.\n",
        "            input_filters = input_filters if i == 0 else round_filters(input_filters, widthi_multiplier)\n",
        "            output_filters = round_filters(output_filters, widthi_multiplier)\n",
        "            num_repeat= num_repeat if i == 0 or i == len(mb_block_settings) - 1  else round_repeats(num_repeat, depth_multiplier)\n",
        "\n",
        "            # The first block needs to take care of stride and filter size increase.\n",
        "            stage.append(MBConvBlock(input_filters, output_filters, kernal_size, stride, expand_ratio, se_ratio, has_se=False))\n",
        "            if num_repeat > 1:\n",
        "                input_filters = output_filters\n",
        "                stride = 1\n",
        "            for _ in range(num_repeat - 1):\n",
        "                stage.append(MBConvBlock(input_filters, output_filters, kernal_size, stride, expand_ratio, se_ratio, has_se=False))\n",
        "\n",
        "            self.blocks.append(stage)\n",
        "\n",
        "        # Head\n",
        "        in_channels = round_filters(mb_block_settings[-1][5], widthi_multiplier)\n",
        "        out_channels = 1280\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(num_features=out_channels, momentum=momentum, eps=epsilon),\n",
        "            nn.ReLU6(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        if dropout_rate > 0:\n",
        "            self.dropout = nn.Dropout(dropout_rate)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "        self.fc = torch.nn.Linear(out_channels, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        # print(x.shape)\n",
        "        x = self.stem(x)\n",
        "        idx = 0\n",
        "        for i, stage in enumerate(self.blocks):\n",
        "            for block in stage:\n",
        "                drop_connect_rate = self.drop_connect_rate\n",
        "                if drop_connect_rate:\n",
        "                    drop_connect_rate *= float(idx) / len(self.blocks)\n",
        "                x = block(x, drop_connect_rate)\n",
        "                idx +=1\n",
        "            if i in [1, 2, 3, 6]:\n",
        "                features.append(x)\n",
        "            # print(f\"After block{i}\", x.shape)\n",
        "        x = self.head(x)\n",
        "        # print(x.shape)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x, features\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.size(1)\n",
        "                m.weight.data.normal_(0, 1.0/float(n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def load_pretrain(self, path):\n",
        "        state_dict = torch.load(path)\n",
        "        self.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def build_efficientnet_lite(name, num_classes):\n",
        "    width_coefficient, depth_coefficient, _, dropout_rate = efficientnet_lite_params[name]\n",
        "    model = EfficientNetLite(width_coefficient, depth_coefficient, num_classes, 0.2, dropout_rate)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_checkpoint(net, checkpoint):\n",
        "    from collections import OrderedDict\n",
        "\n",
        "    temp = OrderedDict()\n",
        "    if 'state_dict' in checkpoint:\n",
        "        checkpoint = dict(checkpoint['state_dict'])\n",
        "    for k in checkpoint:\n",
        "        k2 = 'module.'+k if not k.startswith('module.') else k\n",
        "        temp[k2] = checkpoint[k]\n",
        "\n",
        "    net.load_state_dict(temp, strict=True)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model_name = 'efficientnet_lite1'\n",
        "    model = build_efficientnet_lite(model_name, 1000)\n",
        "    # model.eval()\n",
        "    # x = torch.randn(1, 3, 256, 256)\n",
        "    # x, features = model(x)\n",
        "    # print(features)\n",
        "    # for f in features:\n",
        "    #     print(f.shape)\n",
        "    #     print(f.shape[1])\n",
        "    use_gpu = False\n",
        "    if torch.cuda.is_available():\n",
        "        use_gpu = True\n",
        "    state = torch.load('efficientnet_lite1.pth', map_location='cpu')\n",
        "    model.load_state_dict(state, strict=True)\n",
        "    model.eval()\n",
        "    # state_dict = torch.load(\"efficientnet_lite1.pth\", map_location=\"cpu\")\n",
        "    # model.load_state_dict(state_dict, strict=True)\n",
        "    # model.eval()\n",
        "    # checkpoint = torch.load(\"efficientnet_lite1.pth\")\n",
        "    # model.load_state_dict(checkpoint[\"state_dict\"], strict=True)\n",
        "    # load_checkpoint(model, checkpoint)\n",
        "\n",
        "    # from utils.flops_counter import get_model_complexity_info\n",
        "    #\n",
        "    # wh = efficientnet_lite_params[model_name][2]\n",
        "    # input_shape = (3, wh, wh)\n",
        "    # flops, params = get_model_complexity_info(model, input_shape)\n",
        "    # split_line = '=' * 30\n",
        "    # print(f'{split_line}\\nInput shape: {input_shape}\\n'\n",
        "    #       f'Flops: {flops}\\nParams: {params}\\n{split_line}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLMYphIvVu5X",
        "outputId": "4fe9924f-6405-42c0-ca65-8f49424bacf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing efficient_net.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/RangiLyu/EfficientNet-Lite/releases/download/v1.0/efficientnet_lite1.pth\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdaEKrdTWtuP",
        "outputId": "4f87cc6a-60fc-4b74-b93d-81b7d6cf3156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-23 15:21:52--  https://github.com/RangiLyu/EfficientNet-Lite/releases/download/v1.0/efficientnet_lite1.pth\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/325792291/b5b21a00-4ba4-11eb-8ce6-e4e5f5720061?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250623%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250623T152152Z&X-Amz-Expires=1800&X-Amz-Signature=24a5da12c96f7c6a8f4f72420208556c52823f0ec5d164974dfd2dae7daf2705&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Defficientnet_lite1.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-06-23 15:21:52--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/325792291/b5b21a00-4ba4-11eb-8ce6-e4e5f5720061?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250623%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250623T152152Z&X-Amz-Expires=1800&X-Amz-Signature=24a5da12c96f7c6a8f4f72420208556c52823f0ec5d164974dfd2dae7daf2705&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Defficientnet_lite1.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21947068 (21M) [application/octet-stream]\n",
            "Saving to: ‘efficientnet_lite1.pth’\n",
            "\n",
            "efficientnet_lite1. 100%[===================>]  20.93M  20.6MB/s    in 1.0s    \n",
            "\n",
            "2025-06-23 15:21:54 (20.6 MB/s) - ‘efficientnet_lite1.pth’ saved [21947068/21947068]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile generator.py\n",
        "\"\"\"\n",
        "Generator architecture and code taken from \"Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis\" (arxiv.org/abs/2101.04775) and github.com/odegeasslbc/FastGAN-pytorch, respectively.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import spectral_norm\n",
        "from utils import weights_init\n",
        "\n",
        "\n",
        "def conv2d(*args, **kwargs):\n",
        "    return spectral_norm(nn.Conv2d(*args, **kwargs))\n",
        "\n",
        "\n",
        "def convTranspose2d(*args, **kwargs):\n",
        "    return spectral_norm(nn.ConvTranspose2d(*args, **kwargs))\n",
        "\n",
        "\n",
        "def batchNorm2d(*args, **kwargs):\n",
        "    return nn.BatchNorm2d(*args, **kwargs)\n",
        "\n",
        "\n",
        "def linear(*args, **kwargs):\n",
        "    return spectral_norm(nn.Linear(*args, **kwargs))\n",
        "\n",
        "\n",
        "class GLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        nc = x.size(1)\n",
        "        assert nc % 2 == 0, 'channels dont divide 2!'\n",
        "        nc = int(nc / 2)\n",
        "        return x[:, :nc] * torch.sigmoid(x[:, nc:])\n",
        "\n",
        "\n",
        "class NoiseInjection(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
        "\n",
        "    def forward(self, feat, noise=None):\n",
        "        if noise is None:\n",
        "            batch, _, height, width = feat.shape\n",
        "            noise = torch.randn(batch, 1, height, width).to(feat.device)\n",
        "\n",
        "        return feat + self.weight * noise\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, feat):\n",
        "        return feat * torch.sigmoid(feat)\n",
        "\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super().__init__()\n",
        "\n",
        "        self.main = nn.Sequential(nn.AdaptiveAvgPool2d(4),\n",
        "                                  conv2d(ch_in, ch_out, 4, 1, 0, bias=False), Swish(),\n",
        "                                  conv2d(ch_out, ch_out, 1, 1, 0, bias=False), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, feat_small, feat_big):\n",
        "        return feat_big * self.main(feat_small)\n",
        "\n",
        "\n",
        "class InitLayer(nn.Module):\n",
        "    def __init__(self, nz, channel):\n",
        "        super().__init__()\n",
        "        self.init = nn.Sequential(\n",
        "            convTranspose2d(nz, channel * 2, 4, 1, 0, bias=False),\n",
        "            batchNorm2d(channel * 2), GLU())\n",
        "\n",
        "    def forward(self, noise):\n",
        "        noise = noise.view(noise.shape[0], -1, 1, 1)\n",
        "        return self.init(noise)\n",
        "\n",
        "\n",
        "def UpBlock(in_planes, out_planes):\n",
        "    block = nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "        conv2d(in_planes, out_planes * 2, 3, 1, 1, bias=False),\n",
        "        batchNorm2d(out_planes * 2), GLU())\n",
        "    return block\n",
        "\n",
        "\n",
        "def UpBlockComp(in_planes, out_planes):\n",
        "    block = nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "        conv2d(in_planes, out_planes * 2, 3, 1, 1, bias=False),\n",
        "        NoiseInjection(),\n",
        "        batchNorm2d(out_planes * 2), GLU(),\n",
        "        conv2d(out_planes, out_planes * 2, 3, 1, 1, bias=False),\n",
        "        NoiseInjection(),\n",
        "        batchNorm2d(out_planes * 2), GLU()\n",
        "    )\n",
        "    return block\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngf=64, nz=100, nc=3, im_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        nfc_multi = {4: 16, 8: 8, 16: 4, 32: 2, 64: 2, 128: 1, 256: 0.5, 512: 0.25, 1024: 0.125}\n",
        "        nfc = {}\n",
        "        for k, v in nfc_multi.items():\n",
        "            nfc[k] = int(v * ngf)\n",
        "\n",
        "        self.im_size = im_size\n",
        "\n",
        "        self.init = InitLayer(nz, channel=nfc[4])\n",
        "\n",
        "        self.feat_8 = UpBlockComp(nfc[4], nfc[8])\n",
        "        self.feat_16 = UpBlock(nfc[8], nfc[16])\n",
        "        self.feat_32 = UpBlockComp(nfc[16], nfc[32])\n",
        "        self.feat_64 = UpBlock(nfc[32], nfc[64])\n",
        "        self.feat_128 = UpBlockComp(nfc[64], nfc[128])\n",
        "        self.feat_256 = UpBlock(nfc[128], nfc[256])\n",
        "\n",
        "        self.se_64 = SEBlock(nfc[4], nfc[64])\n",
        "        self.se_128 = SEBlock(nfc[8], nfc[128])\n",
        "        self.se_256 = SEBlock(nfc[16], nfc[256])\n",
        "\n",
        "        self.to_big = conv2d(nfc[im_size], nc, 3, 1, 1, bias=False)\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        feat_4 = self.init(x)\n",
        "        feat_8 = self.feat_8(feat_4)\n",
        "        feat_16 = self.feat_16(feat_8)\n",
        "        feat_32 = self.feat_32(feat_16)\n",
        "\n",
        "        feat_64 = self.se_64(feat_4, self.feat_64(feat_32))\n",
        "\n",
        "        feat_128 = self.se_128(feat_8, self.feat_128(feat_64))\n",
        "\n",
        "        feat_256 = self.se_256(feat_16, self.feat_256(feat_128))\n",
        "\n",
        "        return self.to_big(feat_256)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    gen = Generator(im_size=256)\n",
        "    noise = torch.Tensor(3, 100)\n",
        "    print(gen(noise))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIcVmo5ZWtqC",
        "outputId": "2b14ff23-3cb0-4252-a241-e79f5eaa60c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing generator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "import torch\n",
        "\n",
        "\n",
        "def kaiming_init(module):\n",
        "    classname = module.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if \"Conv\" in classname:\n",
        "        try:\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "        except:\n",
        "            pass\n",
        "    elif \"BatchNorm\" in classname:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "def load_checkpoint(net, checkpoint):\n",
        "    from collections import OrderedDict\n",
        "\n",
        "    temp = OrderedDict()\n",
        "    if 'state_dict' in checkpoint:\n",
        "        checkpoint = dict(checkpoint['state_dict'])\n",
        "    for k in checkpoint:\n",
        "        k2 = 'module.'+k if not k.startswith('module.') else k\n",
        "        temp[k2] = checkpoint[k]\n",
        "\n",
        "    net.load_state_dict(temp, strict=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft6SaOw7Wtmq",
        "outputId": "3b09889c-4a21-4bec-c76f-6a97b20032cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision matplotlib tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "duiE7S0LWtiA",
        "outputId": "6d1317e5-0de8-4cfd-c888-1ed77f108a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile projected_gan.py\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import spectral_norm\n",
        "from torchvision import utils as vutils\n",
        "from utils import kaiming_init, load_checkpoint\n",
        "from efficient_net import build_efficientnet_lite\n",
        "from generator import Generator\n",
        "from differentiable_augmentation import DiffAugment\n",
        "from dataset import load_data\n",
        "import argparse\n",
        "import logging\n",
        "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
        "\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super(DownBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(c_in, c_out, 4, 2, 1)\n",
        "        self.bn = nn.BatchNorm2d(c_out)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return self.leaky_relu(x)\n",
        "\n",
        "\n",
        "class MultiScaleDiscriminator(nn.Module):\n",
        "    def __init__(self, c_in, level):\n",
        "        super(MultiScaleDiscriminator, self).__init__()\n",
        "        self.head_conv = spectral_norm(nn.Conv2d(512, 1, 3, 1, 1))\n",
        "\n",
        "        layers = []\n",
        "        if level == 1:\n",
        "            layers = [\n",
        "                DownBlock(c_in, 64),\n",
        "                DownBlock(64, 128),\n",
        "                DownBlock(128, 256),\n",
        "                DownBlock(256, 512),\n",
        "            ]\n",
        "        elif level == 2:\n",
        "            layers = [\n",
        "                DownBlock(c_in, 128),\n",
        "                DownBlock(128, 256),\n",
        "                DownBlock(256, 512),\n",
        "            ]\n",
        "        elif level == 3:\n",
        "            layers = [\n",
        "                DownBlock(c_in, 256),\n",
        "                DownBlock(256, 512),\n",
        "            ]\n",
        "        elif level == 4:\n",
        "            layers = [\n",
        "                DownBlock(c_in, 512),\n",
        "            ]\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        self.optim = Adam(self.model.parameters(), lr=0.0002, betas=(0.0, 0.99))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return self.head_conv(x)\n",
        "\n",
        "class CSM(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation for the proposed Cross-Scale Mixing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, conv3_out_channels):\n",
        "        super(CSM, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(channels, channels, 3, 1, 1)\n",
        "        self.conv3 = nn.Conv2d(channels, conv3_out_channels, 3, 1, 1)\n",
        "\n",
        "        for param in self.conv1.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for param in self.conv3.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.apply(kaiming_init)\n",
        "\n",
        "    def forward(self, high_res, low_res=None):\n",
        "        batch, channels, width, height = high_res.size()\n",
        "        if low_res is None:\n",
        "            # high_res_flatten = rearrange(high_res, \"b c h w -> b c (h w)\")\n",
        "            high_res_flatten = high_res.view(batch, channels, width * height)\n",
        "            high_res = self.conv1(high_res_flatten)\n",
        "            high_res = high_res.view(batch, channels, width, height)\n",
        "            high_res = self.conv3(high_res)\n",
        "            high_res = F.interpolate(high_res, scale_factor=2., mode=\"bilinear\")\n",
        "            return high_res\n",
        "        else:\n",
        "            high_res_flatten = high_res.view(batch, channels, width * height)\n",
        "            high_res = self.conv1(high_res_flatten)\n",
        "            high_res = high_res.view(batch, channels, width, height)\n",
        "            high_res = torch.add(high_res, low_res)\n",
        "            high_res = self.conv3(high_res)\n",
        "            high_res = F.interpolate(high_res, scale_factor=2., mode=\"bilinear\")\n",
        "            return high_res\n",
        "\n",
        "\n",
        "class ProjectedGAN(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(ProjectedGAN, self).__init__()\n",
        "        self.img_size = args.image_size\n",
        "\n",
        "        self.gen = Generator(im_size=args.image_size)\n",
        "        self.gen_optim = Adam(self.gen.parameters(), lr=args.lr, betas=(args.beta1, args.beta2))\n",
        "\n",
        "        self.efficient_net = build_efficientnet_lite(\"efficientnet_lite1\", 1000)\n",
        "        self.efficient_net = nn.DataParallel(self.efficient_net)\n",
        "        checkpoint = torch.load(args.checkpoint_efficient_net)\n",
        "        load_checkpoint(self.efficient_net, checkpoint)\n",
        "        self.efficient_net.eval()\n",
        "\n",
        "        feature_sizes = self.get_feature_channels()\n",
        "\n",
        "        #print(\"CSM Feature Channels:\", csm_feature_channels)\n",
        "        #feature_sizes = csm_feature_channels\n",
        "        self.csms = nn.ModuleList([\n",
        "            CSM(feature_sizes[3], feature_sizes[2]),\n",
        "            CSM(feature_sizes[2], feature_sizes[1]),\n",
        "            CSM(feature_sizes[1], feature_sizes[0]),\n",
        "            CSM(feature_sizes[0], feature_sizes[0]),\n",
        "        ])\n",
        "        # with torch.no_grad():\n",
        "        #     dummy_features = self.csm_forward(dummy_features)\n",
        "        print(\"Feature sizes:\", feature_sizes)\n",
        "        # self.discs = nn.ModuleList([\n",
        "        #    MultiScaleDiscriminator(feature_sizes[0], 1),\n",
        "        #    MultiScaleDiscriminator(feature_sizes[1], 2),\n",
        "        #    MultiScaleDiscriminator(feature_sizes[2], 3),\n",
        "        #    MultiScaleDiscriminator(feature_sizes[3], 4),\n",
        "        # ]) #[::-1]\n",
        "        # self.discs = nn.ModuleList([\n",
        "        #     MultiScaleDiscriminator(c, level)\n",
        "        #     for c, level in zip(feature_sizes[::-1], [1, 2, 3, 4])\n",
        "        # ])\n",
        "        # self.discs = nn.ModuleList([\n",
        "        #     MultiScaleDiscriminator(c, level)\n",
        "        #     for c, level in zip(feature_sizes[::-1], [1, 2, 3, 4])\n",
        "        # ])\n",
        "        disc_feature_sizes = [80, 40, 24, 24]  # Match CSM output channels\n",
        "        self.discs = nn.ModuleList([\n",
        "             MultiScaleDiscriminator(c, level)\n",
        "             for c, level in zip(disc_feature_sizes, [1, 2, 3, 4])\n",
        "        ])\n",
        "\n",
        "        self.latent_dim = args.latent_dim\n",
        "        self.epochs = args.epochs\n",
        "\n",
        "        augmentations = 'color,translation,cutout'\n",
        "        self.DiffAug = DiffAugment(augmentations)\n",
        "        self.diff_aug = args.diff_aug\n",
        "\n",
        "        self.dataset = load_data(args.dataset_path, args.batch_size)\n",
        "        self.log_every = args.log_every\n",
        "        self.ckpt_path = args.checkpoint_path\n",
        "        self.save_all = args.save_all\n",
        "\n",
        "    def csm_forward(self, features):\n",
        "        features = features[::-1]\n",
        "        csm_features = []\n",
        "        for i, csm in enumerate(self.csms):\n",
        "            if i == 0:\n",
        "                d = csm(features[i])\n",
        "                csm_features.append(d)\n",
        "            else:\n",
        "                d = csm(features[i], d)\n",
        "                csm_features.append(d)\n",
        "        return csm_features\n",
        "\n",
        "    def train(self):\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        logging.info(f\"Using device: {device}\")\n",
        "        self.gen.to(device)\n",
        "        for disc in self.discs:\n",
        "            disc.to(device)\n",
        "        for csm in self.csms:\n",
        "            csm.to(device)\n",
        "        self.efficient_net.to(device)\n",
        "        for epoch in range(self.epochs):\n",
        "            logging.info(f\"Starting epoch {epoch+1}\")\n",
        "            for i, (real_imgs, _) in enumerate(self.dataset):\n",
        "                real_imgs = real_imgs.to(device)\n",
        "                z = torch.randn(real_imgs.shape[0], self.latent_dim)\n",
        "                z = z.to(device)\n",
        "\n",
        "                gen_imgs_disc = self.gen(z).detach()\n",
        "                if self.diff_aug:\n",
        "                    gen_imgs_disc = self.DiffAug.forward(gen_imgs_disc)\n",
        "                    real_imgs = self.DiffAug.forward(real_imgs)\n",
        "\n",
        "                # get efficient net features\n",
        "                _, features_fake = self.efficient_net(gen_imgs_disc)\n",
        "                _, features_real = self.efficient_net(real_imgs)\n",
        "\n",
        "                # feed efficient net features through CSM\n",
        "                features_real = self.csm_forward(features_real)\n",
        "                features_fake = self.csm_forward(features_fake)\n",
        "                features_real = features_real[::-1]\n",
        "                features_fake = features_fake[::-1]\n",
        "\n",
        "\n",
        "                # Train Discriminators:\n",
        "                disc_losses = []\n",
        "                for feature_real, feature_fake, disc in zip(features_real[::-1], features_fake[::-1], self.discs):\n",
        "                    disc.optim.zero_grad()\n",
        "                    y_hat_real = disc(feature_real)  # Cx4x4\n",
        "                    y_hat_fake = disc(feature_fake)  # Cx4x4\n",
        "                    y_hat_real = y_hat_real.sum(1)  # sum along channels axis (is 1 anyways, however it still removes the unnecessary axis)\n",
        "                    y_hat_fake = y_hat_fake.sum(1)\n",
        "                    loss_real = torch.mean(F.relu(1. - y_hat_real))\n",
        "                    loss_fake = torch.mean(F.relu(1. + y_hat_fake))\n",
        "                    disc_loss = loss_real + loss_fake\n",
        "                    disc_loss.backward(retain_graph=True)\n",
        "                    disc.optim.step()\n",
        "                    disc_losses.append(disc_loss.cpu().detach().numpy())\n",
        "\n",
        "                # Train Generator:\n",
        "                z = torch.randn(real_imgs.shape[0], self.latent_dim)\n",
        "                z = z.to(device)\n",
        "                # z = torch.Tensor(np.random.randn(real_imgs.shape[0], self.latent_dim))\n",
        "                # while np.any(np.isnan(z.numpy())):\n",
        "                #     logging.info(\"Recreating z because it has NaN values in it.\")\n",
        "                #     z = torch.Tensor(np.random.randn(real_imgs.shape[0], self.latent_dim))\n",
        "                gen_imgs_gen = self.gen(z)\n",
        "\n",
        "                if self.diff_aug:\n",
        "                    gen_imgs_gen = self.DiffAug.forward(gen_imgs_gen)\n",
        "\n",
        "                # get efficient net features\n",
        "                _, features_fake = self.efficient_net(gen_imgs_gen)\n",
        "\n",
        "                # feed efficient net features through CSM\n",
        "                features_fake = self.csm_forward(features_fake)\n",
        "\n",
        "                gen_loss = 0.\n",
        "                self.gen_optim.zero_grad()\n",
        "                for feature_fake, disc in zip(features_fake, self.discs):\n",
        "                    y_hat = disc(feature_fake)\n",
        "                    y_hat = y_hat.sum(1)\n",
        "                    gen_loss = -torch.mean(y_hat)\n",
        "                gen_loss.backward()\n",
        "                self.gen_optim.step()\n",
        "\n",
        "                if i % self.log_every == 0:\n",
        "                    #print(f\"[Epoch {epoch+1}/{self.epochs}] [Batch {i}/16] \"f\"D_loss: {avg_disc_loss:.4f} G_loss: {gen_loss.item():.4f}\")\n",
        "                    path = os.path.join(self.ckpt_path, str(epoch))\n",
        "                    #os.mkdir(path,exist_ok=True)\n",
        "                    os.makedirs(path, exist_ok=True)  #Made changes\n",
        "                    with torch.no_grad():\n",
        "                        vutils.save_image(gen_imgs_gen.add(1).mul(0.5), os.path.join(path, f'/{epoch}_{i}.jpg'), nrow=4)\n",
        "                    #logging.info(f\"Iteration {i}: Gen Loss = {gen_loss}, Disc Loss = {disc_losses}.\")\n",
        "                    avg_disc_loss = sum(disc_losses) / len(disc_losses)\n",
        "                    print(f\"[Epoch {epoch+1}/{self.epochs}] [Batch {i}] D_loss: {avg_disc_loss:.4f} G_loss: {gen_loss.item():.4f}\")\n",
        "\n",
        "                    logging.info(f\"Iteration {i}: Gen Loss = {gen_loss}, Disc Loss = {disc_losses}.\")\n",
        "                    torch.save(self.gen.state_dict(), os.path.join(path, \"Generator\"))\n",
        "                    if self.save_all:\n",
        "                        for j in range(len(self.discs)):\n",
        "                            torch.save(self.discs[j].state_dict(), os.path.join(path, f\"Discriminator_{j}\"))\n",
        "                            torch.save(self.csms[j].state_dict(), os.path.join(path, f\"CSM_{j}\"))\n",
        "\n",
        "    def get_feature_channels(self):\n",
        "        sample = torch.randn(1, 3, self.img_size, self.img_size)\n",
        "        _, features = self.efficient_net(sample)\n",
        "        return [f.shape[1] for f in features]\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description=\"ProjectedGAN\")\n",
        "    parser.add_argument('--batch-size', type=int, default=32, help='input batch size for training (default: 32)')\n",
        "    parser.add_argument('--epochs', type=int, default=50, metavar='N', help='number of epochs to train (default: 50)')\n",
        "    parser.add_argument('--lr', type=float, default=0.0002, metavar='LR', help='learning rate (default: 0.0002)')\n",
        "    parser.add_argument('--beta1', type=float, default=0.0, metavar='lambda', help='Adam beta param (default: 0.0)')\n",
        "    parser.add_argument('--beta2', type=float, default=0.999, metavar='lambda', help='Adam beta param (default: 0.999)')\n",
        "    parser.add_argument('--latent-dim', type=int, default=100, help='Latent dimension for generator (default: 100)')\n",
        "    parser.add_argument('--diff-aug', type=bool, default=True, help='Apply differentiable augmentation to both discriminator and generator (default: True)')\n",
        "    parser.add_argument('--checkpoint-path', type=str, default=\"/checkpoints\", metavar='Path', help='Path for checkpointing (default: /checkpoints)')\n",
        "    parser.add_argument('--save-all', type=bool, default=False, help='Saves all discriminator, all CSMs and generator if True, only the generator otherwise (default: False)')\n",
        "    parser.add_argument('--checkpoint-efficient-net', type=str, default=\"efficientnet_lite1.pth\", metavar='Path', help='Path for EfficientNet checkpoint (default: efficientnet_lite1.pth)')\n",
        "    parser.add_argument('--log-every', type=int, default=100, help='How often model will be saved, generated images will be saved etc. (default: 100)')\n",
        "    parser.add_argument('--dataset-path', type=str, default='/data', metavar='Path', help='Path to data (default: /data)')\n",
        "    parser.add_argument('--image-size', type=int, default=256, help='Size of images in dataset (default: 256)')\n",
        "    args = parser.parse_args()\n",
        "    Projected_GAN = ProjectedGAN(args)\n",
        "    Projected_GAN.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_PzTfnhWta6",
        "outputId": "c6c58963-8be2-4a52-e863-564f7a244c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting projected_gan.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python dataset.py"
      ],
      "metadata": {
        "id": "pkSPLb0xXJ2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python differentiable_augmentation.py"
      ],
      "metadata": {
        "id": "SNHI1Ib4XJqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python efficient_net.py"
      ],
      "metadata": {
        "id": "ytbNQK19XJig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python generator.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dAsEWuHXJaH",
        "outputId": "05665e92-01a5-479f-8ad2-cb6da48fcd32",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-3.4098e-02, -2.2817e-02, -2.6630e-03,  ...,  8.8804e-03,\n",
            "            3.1858e-03, -2.4989e-03],\n",
            "          [-4.3550e-02, -5.8841e-02, -2.4447e-02,  ..., -2.5357e-02,\n",
            "           -2.6932e-02, -1.8417e-02],\n",
            "          [-5.6157e-02, -7.4284e-02, -3.7967e-02,  ..., -7.4404e-02,\n",
            "           -5.3912e-02, -4.3422e-02],\n",
            "          ...,\n",
            "          [ 1.6785e-02,  1.6225e-02,  3.3099e-02,  ...,  4.3141e-02,\n",
            "           -1.2606e-02, -2.8948e-02],\n",
            "          [ 7.6376e-03, -1.5637e-03,  1.6621e-02,  ...,  4.1237e-02,\n",
            "            6.0475e-03, -1.5482e-02],\n",
            "          [ 1.2902e-02,  1.8094e-02,  2.2166e-02,  ...,  3.2359e-02,\n",
            "            4.7385e-03, -1.5918e-03]],\n",
            "\n",
            "         [[-8.6014e-03, -1.0764e-02,  1.4003e-03,  ..., -5.6154e-02,\n",
            "           -4.3058e-02, -1.9028e-02],\n",
            "          [-9.5674e-03, -2.3650e-02, -8.5543e-04,  ..., -3.9774e-02,\n",
            "           -4.9389e-02, -3.0205e-02],\n",
            "          [-5.0925e-03, -4.1593e-02, -2.3413e-03,  ..., -2.3618e-02,\n",
            "           -3.1150e-02, -2.3541e-02],\n",
            "          ...,\n",
            "          [-4.0071e-02, -4.1118e-02, -4.4632e-02,  ..., -3.8120e-02,\n",
            "            8.0754e-03, -2.6214e-02],\n",
            "          [-1.3167e-02, -1.7107e-02, -3.1095e-02,  ..., -6.6923e-03,\n",
            "            3.2650e-02,  4.8643e-03],\n",
            "          [-9.0099e-03, -2.7259e-02, -2.8612e-02,  ...,  2.4708e-03,\n",
            "            2.8260e-02, -6.5697e-04]],\n",
            "\n",
            "         [[ 1.5956e-03,  2.2910e-02,  4.5596e-02,  ..., -1.5960e-02,\n",
            "           -2.0613e-02, -8.8217e-03],\n",
            "          [-9.3391e-03,  4.8600e-03,  3.1739e-02,  ..., -2.2332e-02,\n",
            "           -2.5629e-02, -7.0410e-03],\n",
            "          [ 4.4988e-03, -9.0307e-04,  2.2071e-02,  ...,  1.0328e-02,\n",
            "            2.2822e-02, -8.9009e-03],\n",
            "          ...,\n",
            "          [-2.8364e-02, -1.3036e-02, -2.0771e-02,  ..., -4.0218e-02,\n",
            "           -4.7589e-02, -2.7270e-02],\n",
            "          [-2.5863e-02, -1.8094e-02, -1.3658e-02,  ..., -5.9432e-03,\n",
            "           -3.3919e-02, -2.0789e-02],\n",
            "          [ 5.2979e-03, -1.4360e-03,  5.6199e-03,  ...,  1.2613e-02,\n",
            "           -8.8425e-03, -1.9634e-02]]],\n",
            "\n",
            "\n",
            "        [[[-6.9180e-03, -5.9851e-03, -6.9929e-03,  ..., -7.1137e-03,\n",
            "           -1.8235e-03, -1.8244e-03],\n",
            "          [-6.5057e-05, -6.3553e-03, -9.8741e-03,  ..., -5.0467e-03,\n",
            "           -1.7492e-03, -4.9468e-03],\n",
            "          [-2.9948e-03, -5.2736e-03, -1.0962e-02,  ..., -9.9911e-03,\n",
            "           -5.7005e-03, -6.2293e-03],\n",
            "          ...,\n",
            "          [-1.0182e-03, -4.9022e-03,  6.9521e-04,  ..., -2.8415e-03,\n",
            "           -9.5179e-04, -1.0764e-02],\n",
            "          [-2.8478e-04, -6.4850e-03, -2.0588e-03,  ..., -3.2186e-03,\n",
            "           -7.0952e-04, -8.9649e-03],\n",
            "          [-1.7370e-03, -3.4996e-03,  2.5804e-03,  ..., -6.3038e-03,\n",
            "           -6.5322e-03, -3.5372e-03]],\n",
            "\n",
            "         [[-1.1388e-02, -8.9474e-03, -1.1754e-02,  ..., -1.0964e-02,\n",
            "           -6.1246e-03, -6.1534e-03],\n",
            "          [-1.2557e-02, -6.0073e-03, -5.9594e-03,  ..., -7.5496e-03,\n",
            "           -4.3834e-04, -6.0185e-03],\n",
            "          [-1.2022e-02, -6.0045e-03, -5.8217e-03,  ..., -2.5860e-03,\n",
            "            7.5872e-03,  2.7148e-04],\n",
            "          ...,\n",
            "          [-1.3167e-02, -7.3379e-03, -4.9637e-03,  ..., -4.1914e-03,\n",
            "            1.5198e-03, -3.8197e-03],\n",
            "          [-6.2084e-03,  1.8940e-03,  1.8199e-03,  ...,  8.4498e-04,\n",
            "            6.3251e-03, -2.1546e-04],\n",
            "          [-3.8369e-03,  2.2996e-03, -1.8125e-04,  ..., -4.0979e-03,\n",
            "            7.5789e-04, -1.7877e-03]],\n",
            "\n",
            "         [[-1.2053e-02, -4.8292e-03, -6.6145e-03,  ..., -1.1553e-02,\n",
            "           -1.2057e-02, -1.3088e-03],\n",
            "          [-1.5587e-02, -7.9689e-03, -4.7671e-03,  ..., -1.5829e-02,\n",
            "           -1.9723e-02, -6.1475e-03],\n",
            "          [-1.0815e-02, -7.0118e-03, -1.6620e-03,  ..., -9.0832e-03,\n",
            "           -1.4889e-02, -3.0533e-03],\n",
            "          ...,\n",
            "          [-1.5746e-02, -1.1857e-02, -1.3348e-02,  ..., -1.1331e-02,\n",
            "           -1.4508e-02, -5.5390e-03],\n",
            "          [-1.3433e-02, -1.7767e-03, -2.0769e-03,  ..., -2.4730e-03,\n",
            "           -6.4914e-03, -8.6374e-04],\n",
            "          [-7.3806e-04,  6.2343e-03,  2.5895e-03,  ...,  7.5074e-03,\n",
            "            3.0709e-03, -3.4389e-03]]],\n",
            "\n",
            "\n",
            "        [[[-6.8908e-03, -5.9902e-03, -6.9796e-03,  ..., -7.0723e-03,\n",
            "           -1.8151e-03, -1.8135e-03],\n",
            "          [-6.0119e-05, -6.3728e-03, -9.8827e-03,  ..., -5.0421e-03,\n",
            "           -1.7640e-03, -4.9619e-03],\n",
            "          [-2.9455e-03, -5.2383e-03, -1.0859e-02,  ..., -9.9906e-03,\n",
            "           -5.7153e-03, -6.2294e-03],\n",
            "          ...,\n",
            "          [-1.0209e-03, -4.9262e-03,  6.8499e-04,  ..., -2.7641e-03,\n",
            "           -8.9583e-04, -1.0688e-02],\n",
            "          [-2.8275e-04, -6.4942e-03, -2.0775e-03,  ..., -3.1474e-03,\n",
            "           -6.5070e-04, -8.8970e-03],\n",
            "          [-1.7297e-03, -3.4939e-03,  2.5665e-03,  ..., -6.2704e-03,\n",
            "           -6.4963e-03, -3.4946e-03]],\n",
            "\n",
            "         [[-1.1395e-02, -8.9590e-03, -1.1749e-02,  ..., -1.0975e-02,\n",
            "           -6.1546e-03, -6.1720e-03],\n",
            "          [-1.2551e-02, -6.0046e-03, -5.9308e-03,  ..., -7.5546e-03,\n",
            "           -4.6845e-04, -6.0229e-03],\n",
            "          [-1.2013e-02, -5.9832e-03, -5.7958e-03,  ..., -2.6418e-03,\n",
            "            7.5269e-03,  2.6129e-04],\n",
            "          ...,\n",
            "          [-1.3198e-02, -7.3563e-03, -4.9612e-03,  ..., -4.2321e-03,\n",
            "            1.4690e-03, -3.8474e-03],\n",
            "          [-6.2340e-03,  1.8556e-03,  1.8158e-03,  ...,  8.3628e-04,\n",
            "            6.3157e-03, -2.2014e-04],\n",
            "          [-3.8542e-03,  2.2814e-03, -1.8203e-04,  ..., -4.1136e-03,\n",
            "            7.3876e-04, -1.8037e-03]],\n",
            "\n",
            "         [[-1.2078e-02, -4.8940e-03, -6.6912e-03,  ..., -1.1570e-02,\n",
            "           -1.2062e-02, -1.3179e-03],\n",
            "          [-1.5593e-02, -8.0195e-03, -4.7980e-03,  ..., -1.5810e-02,\n",
            "           -1.9710e-02, -6.1415e-03],\n",
            "          [-1.0827e-02, -7.0747e-03, -1.7186e-03,  ..., -9.0958e-03,\n",
            "           -1.4913e-02, -3.0618e-03],\n",
            "          ...,\n",
            "          [-1.5728e-02, -1.1812e-02, -1.3318e-02,  ..., -1.1284e-02,\n",
            "           -1.4475e-02, -5.4909e-03],\n",
            "          [-1.3413e-02, -1.7323e-03, -2.0424e-03,  ..., -2.4249e-03,\n",
            "           -6.4642e-03, -8.2952e-04],\n",
            "          [-7.4679e-04,  6.2236e-03,  2.5729e-03,  ...,  7.5286e-03,\n",
            "            3.0831e-03, -3.4245e-03]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python utils.py"
      ],
      "metadata": {
        "id": "YEZZIonvXJQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python projected_gan.py \\\n",
        "  --batch-size 16 \\\n",
        "  --epochs 50 \\\n",
        "  --lr 0.0002 \\\n",
        "  --latent-dim 100 \\\n",
        "  --diff-aug True \\\n",
        "  --checkpoint-path ./checkpoints \\\n",
        "  --checkpoint-efficient-net efficientnet_lite1.pth \\\n",
        "  --dataset-path ./organized_skin_lesions \\\n",
        "  --image-size 256 \\\n",
        "  --log-every 10\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQtOp3MnXo5I",
        "outputId": "fbc5d708-f3d8-477d-ffe9-e43ba07b5cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/projected_gan.py\", line 8, in <module>\n",
            "    from torchvision import utils as vutils\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\", line 10, in <module>\n",
            "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/__init__.py\", line 17, in <module>\n",
            "    from . import detection, optical_flow, quantization, segmentation, video\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/__init__.py\", line 1, in <module>\n",
            "    from .faster_rcnn import *\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/faster_rcnn.py\", line 19, in <module>\n",
            "    from .roi_heads import RoIHeads\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1069, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 729, in _compile_bytecode\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile generate_samples.py\n",
        "import torch\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "from generator import Generator\n",
        "import glob\n",
        "\n",
        "# Configs\n",
        "nz = 100\n",
        "image_size = 256\n",
        "checkpoint_dir = \"checkpoints\"  # Directory containing all checkpoints\n",
        "output_base_dir = \"synthetic_samples\"\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Find all generator checkpoints\n",
        "checkpoint_paths = sorted(glob.glob(f\"{checkpoint_dir}/*/Generator\"))\n",
        "print(f\"Found {len(checkpoint_paths)} checkpoints\")\n",
        "\n",
        "# Create generator model\n",
        "gen = Generator(nz=nz, im_size=image_size).to(device)\n",
        "\n",
        "# Process each checkpoint\n",
        "for checkpoint_path in checkpoint_paths:\n",
        "    # Extract epoch number from path (assumes structure: checkpoints/epoch_X/Generator)\n",
        "    epoch = checkpoint_path.split('/')[1]\n",
        "    output_dir = os.path.join(output_base_dir, f\"epoch_{epoch}\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load weights\n",
        "    gen.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    gen.eval()\n",
        "\n",
        "    # Generate images\n",
        "    num_images = 100  # Generate 100 synthetic images per checkpoint\n",
        "    noise = torch.randn(num_images, nz, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake_images = gen(noise)\n",
        "\n",
        "    # Save all individual images\n",
        "    for i in range(num_images):\n",
        "        save_image(fake_images[i], os.path.join(output_dir, f\"sample_{i+1}.png\"), normalize=True)\n",
        "\n",
        "    print(f\"✅ Generated {num_images} samples for epoch {epoch} at {output_dir}\")\n",
        "\n",
        "print(\"Completed all checkpoint generations!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK21AW6-ZG7_",
        "outputId": "abc2d685-aa9c-41dc-ad0b-fc3a781a04b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing generate_samples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate_samples.py"
      ],
      "metadata": {
        "id": "kdU3IIa1ZfWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fb0e50-0d05-4437-c0d0-3544096b69dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 50 checkpoints\n",
            "✅ Generated 100 samples for epoch 0 at synthetic_samples/epoch_0\n",
            "✅ Generated 100 samples for epoch 1 at synthetic_samples/epoch_1\n",
            "✅ Generated 100 samples for epoch 10 at synthetic_samples/epoch_10\n",
            "✅ Generated 100 samples for epoch 11 at synthetic_samples/epoch_11\n",
            "✅ Generated 100 samples for epoch 12 at synthetic_samples/epoch_12\n",
            "✅ Generated 100 samples for epoch 13 at synthetic_samples/epoch_13\n",
            "✅ Generated 100 samples for epoch 14 at synthetic_samples/epoch_14\n",
            "✅ Generated 100 samples for epoch 15 at synthetic_samples/epoch_15\n",
            "✅ Generated 100 samples for epoch 16 at synthetic_samples/epoch_16\n",
            "✅ Generated 100 samples for epoch 17 at synthetic_samples/epoch_17\n",
            "✅ Generated 100 samples for epoch 18 at synthetic_samples/epoch_18\n",
            "✅ Generated 100 samples for epoch 19 at synthetic_samples/epoch_19\n",
            "✅ Generated 100 samples for epoch 2 at synthetic_samples/epoch_2\n",
            "✅ Generated 100 samples for epoch 20 at synthetic_samples/epoch_20\n",
            "✅ Generated 100 samples for epoch 21 at synthetic_samples/epoch_21\n",
            "✅ Generated 100 samples for epoch 22 at synthetic_samples/epoch_22\n",
            "✅ Generated 100 samples for epoch 23 at synthetic_samples/epoch_23\n",
            "✅ Generated 100 samples for epoch 24 at synthetic_samples/epoch_24\n",
            "✅ Generated 100 samples for epoch 25 at synthetic_samples/epoch_25\n",
            "✅ Generated 100 samples for epoch 26 at synthetic_samples/epoch_26\n",
            "✅ Generated 100 samples for epoch 27 at synthetic_samples/epoch_27\n",
            "✅ Generated 100 samples for epoch 28 at synthetic_samples/epoch_28\n",
            "✅ Generated 100 samples for epoch 29 at synthetic_samples/epoch_29\n",
            "✅ Generated 100 samples for epoch 3 at synthetic_samples/epoch_3\n",
            "✅ Generated 100 samples for epoch 30 at synthetic_samples/epoch_30\n",
            "✅ Generated 100 samples for epoch 31 at synthetic_samples/epoch_31\n",
            "✅ Generated 100 samples for epoch 32 at synthetic_samples/epoch_32\n",
            "✅ Generated 100 samples for epoch 33 at synthetic_samples/epoch_33\n",
            "✅ Generated 100 samples for epoch 34 at synthetic_samples/epoch_34\n",
            "✅ Generated 100 samples for epoch 35 at synthetic_samples/epoch_35\n",
            "✅ Generated 100 samples for epoch 36 at synthetic_samples/epoch_36\n",
            "✅ Generated 100 samples for epoch 37 at synthetic_samples/epoch_37\n",
            "✅ Generated 100 samples for epoch 38 at synthetic_samples/epoch_38\n",
            "✅ Generated 100 samples for epoch 39 at synthetic_samples/epoch_39\n",
            "✅ Generated 100 samples for epoch 4 at synthetic_samples/epoch_4\n",
            "✅ Generated 100 samples for epoch 40 at synthetic_samples/epoch_40\n",
            "✅ Generated 100 samples for epoch 41 at synthetic_samples/epoch_41\n",
            "✅ Generated 100 samples for epoch 42 at synthetic_samples/epoch_42\n",
            "✅ Generated 100 samples for epoch 43 at synthetic_samples/epoch_43\n",
            "✅ Generated 100 samples for epoch 44 at synthetic_samples/epoch_44\n",
            "✅ Generated 100 samples for epoch 45 at synthetic_samples/epoch_45\n",
            "✅ Generated 100 samples for epoch 46 at synthetic_samples/epoch_46\n",
            "✅ Generated 100 samples for epoch 47 at synthetic_samples/epoch_47\n",
            "✅ Generated 100 samples for epoch 48 at synthetic_samples/epoch_48\n",
            "✅ Generated 100 samples for epoch 49 at synthetic_samples/epoch_49\n",
            "✅ Generated 100 samples for epoch 5 at synthetic_samples/epoch_5\n",
            "✅ Generated 100 samples for epoch 6 at synthetic_samples/epoch_6\n",
            "✅ Generated 100 samples for epoch 7 at synthetic_samples/epoch_7\n",
            "✅ Generated 100 samples for epoch 8 at synthetic_samples/epoch_8\n",
            "✅ Generated 100 samples for epoch 9 at synthetic_samples/epoch_9\n",
            "Completed all checkpoint generations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsHyLVPWfmKo",
        "outputId": "4a69e5d9-8c8c-47c1-e2bf-ff1b8516681d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset.py\t\tresized_images\t     skin-cancer-mnist-ham10000.zip\n",
            "kaggle.json\t\tsample_data\t     synthetic_samples.zip\n",
            "organized_skin_lesions\tskin_cancer_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "real_images_path = './organized_skin_lesions'\n",
        "generated_images_path = './synthetic_samples/epoch_4'\n",
        "\n",
        "real_images_count = 0\n",
        "generated_images_count = 0\n",
        "\n",
        "if os.path.exists(real_images_path):\n",
        "    for root, dirs, files in os.walk(real_images_path):\n",
        "        real_images_count += len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "if os.path.exists(generated_images_path):\n",
        "    for root, dirs, files in os.walk(generated_images_path):\n",
        "        generated_images_count += len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "print(f\"Real images: {real_images_count}, Generated images: {generated_images_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d82QjdUgRct",
        "outputId": "bf8658ab-ad94-4241-f471-06e3a1672466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real images: 10015, Generated images: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pytorch-fid if not already installed\n",
        "!pip install pytorch-fid\n",
        "\n",
        "import torch\n",
        "from pytorch_fid import fid_score\n",
        "\n",
        "real_images_path = './resized_images/'\n",
        "generated_images_path = './unzipped_data/epoch_49'  # Use the correct epoch folder\n",
        "\n",
        "\n",
        "real_images = [f for f in os.listdir(real_images_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "generated_images = [f for f in os.listdir(generated_images_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "print(f\"Real images: {len(real_images)}\")\n",
        "print(f\"Generated images: {len(generated_images)}\")\n",
        "\n",
        "\n",
        "batch_size = min(80, len(real_images), len(generated_images))\n",
        "\n",
        "# Calculate FID score\n",
        "fid_value = fid_score.calculate_fid_given_paths(\n",
        "    [real_images_path, generated_images_path],\n",
        "    batch_size=batch_size,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    dims=2048\n",
        ")\n",
        "\n",
        "print(f\"FID score between real and generated images: {fid_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6LC3JV1gqbp",
        "outputId": "ee2a22b5-48a3-4672-c7a0-68bd1f3e6936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-fid\n",
            "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (11.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.1->pytorch-fid) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.1->pytorch-fid) (3.0.2)\n",
            "Downloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pytorch-fid\n",
            "Successfully installed pytorch-fid-0.3.0\n",
            "Real images: 100\n",
            "Generated images: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n",
            "100%|██████████| 91.2M/91.2M [00:02<00:00, 39.0MB/s]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.88it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FID score between real and generated images: 189.69662610669556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "#generated_images_path = './synthetic_samples/epoch_4'\n",
        "generated_images_path = './organized_skin_lesions/akiec'\n",
        "\n",
        "# List all image files in the directory\n",
        "image_files = [f for f in os.listdir(generated_images_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Print size of each image\n",
        "for img_file in image_files:\n",
        "    img_path = os.path.join(generated_images_path, img_file)\n",
        "    try:\n",
        "        with Image.open(img_path) as img:\n",
        "            print(f\"{img_file}: {img.size}\")  # (width, height)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {img_file}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HuzN_0eijI6s",
        "outputId": "96f1f460-97c1-4527-8e2c-f950b28c9e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ISIC_0029715.jpg: (600, 450)\n",
            "ISIC_0028990.jpg: (600, 450)\n",
            "ISIC_0030133.jpg: (600, 450)\n",
            "ISIC_0029563.jpg: (600, 450)\n",
            "ISIC_0027562.jpg: (600, 450)\n",
            "ISIC_0031108.jpg: (600, 450)\n",
            "ISIC_0032356.jpg: (600, 450)\n",
            "ISIC_0026468.jpg: (600, 450)\n",
            "ISIC_0031286.jpg: (600, 450)\n",
            "ISIC_0024539.jpg: (600, 450)\n",
            "ISIC_0025811.jpg: (600, 450)\n",
            "ISIC_0029043.jpg: (600, 450)\n",
            "ISIC_0030953.jpg: (600, 450)\n",
            "ISIC_0030844.jpg: (600, 450)\n",
            "ISIC_0030655.jpg: (600, 450)\n",
            "ISIC_0026848.jpg: (600, 450)\n",
            "ISIC_0027084.jpg: (600, 450)\n",
            "ISIC_0025539.jpg: (600, 450)\n",
            "ISIC_0030076.jpg: (600, 450)\n",
            "ISIC_0026702.jpg: (600, 450)\n",
            "ISIC_0026626.jpg: (600, 450)\n",
            "ISIC_0028730.jpg: (600, 450)\n",
            "ISIC_0030375.jpg: (600, 450)\n",
            "ISIC_0030714.jpg: (600, 450)\n",
            "ISIC_0028190.jpg: (600, 450)\n",
            "ISIC_0029460.jpg: (600, 450)\n",
            "ISIC_0024800.jpg: (600, 450)\n",
            "ISIC_0029659.jpg: (600, 450)\n",
            "ISIC_0029900.jpg: (600, 450)\n",
            "ISIC_0028816.jpg: (600, 450)\n",
            "ISIC_0029041.jpg: (600, 450)\n",
            "ISIC_0032422.jpg: (600, 450)\n",
            "ISIC_0025992.jpg: (600, 450)\n",
            "ISIC_0027650.jpg: (600, 450)\n",
            "ISIC_0029549.jpg: (600, 450)\n",
            "ISIC_0026857.jpg: (600, 450)\n",
            "ISIC_0032397.jpg: (600, 450)\n",
            "ISIC_0026171.jpg: (600, 450)\n",
            "ISIC_0031570.jpg: (600, 450)\n",
            "ISIC_0025577.jpg: (600, 450)\n",
            "ISIC_0026729.jpg: (600, 450)\n",
            "ISIC_0032206.jpg: (600, 450)\n",
            "ISIC_0030158.jpg: (600, 450)\n",
            "ISIC_0028393.jpg: (600, 450)\n",
            "ISIC_0032897.jpg: (600, 450)\n",
            "ISIC_0032203.jpg: (600, 450)\n",
            "ISIC_0025264.jpg: (600, 450)\n",
            "ISIC_0030245.jpg: (600, 450)\n",
            "ISIC_0029360.jpg: (600, 450)\n",
            "ISIC_0033811.jpg: (600, 450)\n",
            "ISIC_0024707.jpg: (600, 450)\n",
            "ISIC_0031119.jpg: (600, 450)\n",
            "ISIC_0025953.jpg: (600, 450)\n",
            "ISIC_0030591.jpg: (600, 450)\n",
            "ISIC_0027254.jpg: (600, 450)\n",
            "ISIC_0025069.jpg: (600, 450)\n",
            "ISIC_0026319.jpg: (600, 450)\n",
            "ISIC_0024654.jpg: (600, 450)\n",
            "ISIC_0033550.jpg: (600, 450)\n",
            "ISIC_0030463.jpg: (600, 450)\n",
            "ISIC_0027829.jpg: (600, 450)\n",
            "ISIC_0028795.jpg: (600, 450)\n",
            "ISIC_0029315.jpg: (600, 450)\n",
            "ISIC_0030491.jpg: (600, 450)\n",
            "ISIC_0026927.jpg: (600, 450)\n",
            "ISIC_0025471.jpg: (600, 450)\n",
            "ISIC_0029371.jpg: (600, 450)\n",
            "ISIC_0029930.jpg: (600, 450)\n",
            "ISIC_0026411.jpg: (600, 450)\n",
            "ISIC_0026212.jpg: (600, 450)\n",
            "ISIC_0031381.jpg: (600, 450)\n",
            "ISIC_0026388.jpg: (600, 450)\n",
            "ISIC_0029059.jpg: (600, 450)\n",
            "ISIC_0027536.jpg: (600, 450)\n",
            "ISIC_0026650.jpg: (600, 450)\n",
            "ISIC_0024646.jpg: (600, 450)\n",
            "ISIC_0027303.jpg: (600, 450)\n",
            "ISIC_0031918.jpg: (600, 450)\n",
            "ISIC_0028372.jpg: (600, 450)\n",
            "ISIC_0029279.jpg: (600, 450)\n",
            "ISIC_0027767.jpg: (600, 450)\n",
            "ISIC_0031927.jpg: (600, 450)\n",
            "ISIC_0027795.jpg: (600, 450)\n",
            "ISIC_0028517.jpg: (600, 450)\n",
            "ISIC_0029309.jpg: (600, 450)\n",
            "ISIC_0031940.jpg: (600, 450)\n",
            "ISIC_0031040.jpg: (600, 450)\n",
            "ISIC_0033413.jpg: (600, 450)\n",
            "ISIC_0029002.jpg: (600, 450)\n",
            "ISIC_0025319.jpg: (600, 450)\n",
            "ISIC_0026992.jpg: (600, 450)\n",
            "ISIC_0029840.jpg: (600, 450)\n",
            "ISIC_0028872.jpg: (600, 450)\n",
            "ISIC_0030986.jpg: (600, 450)\n",
            "ISIC_0028063.jpg: (600, 450)\n",
            "ISIC_0026152.jpg: (600, 450)\n",
            "ISIC_0029141.jpg: (600, 450)\n",
            "ISIC_0026014.jpg: (600, 450)\n",
            "ISIC_0024522.jpg: (600, 450)\n",
            "ISIC_0030487.jpg: (600, 450)\n",
            "ISIC_0029268.jpg: (600, 450)\n",
            "ISIC_0025957.jpg: (600, 450)\n",
            "ISIC_0030877.jpg: (600, 450)\n",
            "ISIC_0031044.jpg: (600, 450)\n",
            "ISIC_0033494.jpg: (600, 450)\n",
            "ISIC_0024463.jpg: (600, 450)\n",
            "ISIC_0030803.jpg: (600, 450)\n",
            "ISIC_0025350.jpg: (600, 450)\n",
            "ISIC_0031191.jpg: (600, 450)\n",
            "ISIC_0033869.jpg: (600, 450)\n",
            "ISIC_0026178.jpg: (600, 450)\n",
            "ISIC_0025178.jpg: (600, 450)\n",
            "ISIC_0026575.jpg: (600, 450)\n",
            "ISIC_0027884.jpg: (600, 450)\n",
            "ISIC_0029533.jpg: (600, 450)\n",
            "ISIC_0031852.jpg: (600, 450)\n",
            "ISIC_0027047.jpg: (600, 450)\n",
            "ISIC_0030821.jpg: (600, 450)\n",
            "ISIC_0024946.jpg: (600, 450)\n",
            "ISIC_0031738.jpg: (600, 450)\n",
            "ISIC_0028370.jpg: (600, 450)\n",
            "ISIC_0024450.jpg: (600, 450)\n",
            "ISIC_0024771.jpg: (600, 450)\n",
            "ISIC_0028335.jpg: (600, 450)\n",
            "ISIC_0025696.jpg: (600, 450)\n",
            "ISIC_0026466.jpg: (600, 450)\n",
            "ISIC_0030785.jpg: (600, 450)\n",
            "ISIC_0027184.jpg: (600, 450)\n",
            "ISIC_0029915.jpg: (600, 450)\n",
            "ISIC_0029133.jpg: (600, 450)\n",
            "ISIC_0024562.jpg: (600, 450)\n",
            "ISIC_0028224.jpg: (600, 450)\n",
            "ISIC_0029500.jpg: (600, 450)\n",
            "ISIC_0033084.jpg: (600, 450)\n",
            "ISIC_0027580.jpg: (600, 450)\n",
            "ISIC_0025808.jpg: (600, 450)\n",
            "ISIC_0025790.jpg: (600, 450)\n",
            "ISIC_0032371.jpg: (600, 450)\n",
            "ISIC_0031672.jpg: (600, 450)\n",
            "ISIC_0026083.jpg: (600, 450)\n",
            "ISIC_0026492.jpg: (600, 450)\n",
            "ISIC_0026194.jpg: (600, 450)\n",
            "ISIC_0029860.jpg: (600, 450)\n",
            "ISIC_0027958.jpg: (600, 450)\n",
            "ISIC_0030175.jpg: (600, 450)\n",
            "ISIC_0028132.jpg: (600, 450)\n",
            "ISIC_0024710.jpg: (600, 450)\n",
            "ISIC_0027529.jpg: (600, 450)\n",
            "ISIC_0026765.jpg: (600, 450)\n",
            "ISIC_0033295.jpg: (600, 450)\n",
            "ISIC_0025825.jpg: (600, 450)\n",
            "ISIC_0027719.jpg: (600, 450)\n",
            "ISIC_0030549.jpg: (600, 450)\n",
            "ISIC_0027231.jpg: (600, 450)\n",
            "ISIC_0024517.jpg: (600, 450)\n",
            "ISIC_0027343.jpg: (600, 450)\n",
            "ISIC_0024913.jpg: (600, 450)\n",
            "ISIC_0033456.jpg: (600, 450)\n",
            "ISIC_0027172.jpg: (600, 450)\n",
            "ISIC_0031043.jpg: (600, 450)\n",
            "ISIC_0032437.jpg: (600, 450)\n",
            "ISIC_0027577.jpg: (600, 450)\n",
            "ISIC_0028941.jpg: (600, 450)\n",
            "ISIC_0026981.jpg: (600, 450)\n",
            "ISIC_0031335.jpg: (600, 450)\n",
            "ISIC_0031993.jpg: (600, 450)\n",
            "ISIC_0030707.jpg: (600, 450)\n",
            "ISIC_0027506.jpg: (600, 450)\n",
            "ISIC_0031609.jpg: (600, 450)\n",
            "ISIC_0031922.jpg: (600, 450)\n",
            "ISIC_0033705.jpg: (600, 450)\n",
            "ISIC_0029781.jpg: (600, 450)\n",
            "ISIC_0031228.jpg: (600, 450)\n",
            "ISIC_0031012.jpg: (600, 450)\n",
            "ISIC_0025182.jpg: (600, 450)\n",
            "ISIC_0030143.jpg: (600, 450)\n",
            "ISIC_0027802.jpg: (600, 450)\n",
            "ISIC_0027896.jpg: (600, 450)\n",
            "ISIC_0027615.jpg: (600, 450)\n",
            "ISIC_0028854.jpg: (600, 450)\n",
            "ISIC_0024923.jpg: (600, 450)\n",
            "ISIC_0031292.jpg: (600, 450)\n",
            "ISIC_0028232.jpg: (600, 450)\n",
            "ISIC_0028763.jpg: (600, 450)\n",
            "ISIC_0027447.jpg: (600, 450)\n",
            "ISIC_0025247.jpg: (600, 450)\n",
            "ISIC_0029827.jpg: (600, 450)\n",
            "ISIC_0027334.jpg: (600, 450)\n",
            "ISIC_0026362.jpg: (600, 450)\n",
            "ISIC_0030602.jpg: (600, 450)\n",
            "ISIC_0026149.jpg: (600, 450)\n",
            "ISIC_0032014.jpg: (600, 450)\n",
            "ISIC_0026327.jpg: (600, 450)\n",
            "ISIC_0029462.jpg: (600, 450)\n",
            "ISIC_0024468.jpg: (600, 450)\n",
            "ISIC_0024843.jpg: (600, 450)\n",
            "ISIC_0030242.jpg: (600, 450)\n",
            "ISIC_0027950.jpg: (600, 450)\n",
            "ISIC_0028314.jpg: (600, 450)\n",
            "ISIC_0030344.jpg: (600, 450)\n",
            "ISIC_0033000.jpg: (600, 450)\n",
            "ISIC_0029713.jpg: (600, 450)\n",
            "ISIC_0032277.jpg: (600, 450)\n",
            "ISIC_0028820.jpg: (600, 450)\n",
            "ISIC_0025029.jpg: (600, 450)\n",
            "ISIC_0029067.jpg: (600, 450)\n",
            "ISIC_0028381.jpg: (600, 450)\n",
            "ISIC_0031198.jpg: (600, 450)\n",
            "ISIC_0028158.jpg: (600, 450)\n",
            "ISIC_0030191.jpg: (600, 450)\n",
            "ISIC_0030280.jpg: (600, 450)\n",
            "ISIC_0026040.jpg: (600, 450)\n",
            "ISIC_0032455.jpg: (600, 450)\n",
            "ISIC_0027930.jpg: (600, 450)\n",
            "ISIC_0031421.jpg: (600, 450)\n",
            "ISIC_0024575.jpg: (600, 450)\n",
            "ISIC_0024372.jpg: (600, 450)\n",
            "ISIC_0029634.jpg: (600, 450)\n",
            "ISIC_0030297.jpg: (600, 450)\n",
            "ISIC_0031578.jpg: (600, 450)\n",
            "ISIC_0027753.jpg: (600, 450)\n",
            "ISIC_0028644.jpg: (600, 450)\n",
            "ISIC_0024511.jpg: (600, 450)\n",
            "ISIC_0032329.jpg: (600, 450)\n",
            "ISIC_0031692.jpg: (600, 450)\n",
            "ISIC_0026457.jpg: (600, 450)\n",
            "ISIC_0026522.jpg: (600, 450)\n",
            "ISIC_0027452.jpg: (600, 450)\n",
            "ISIC_0032173.jpg: (600, 450)\n",
            "ISIC_0032854.jpg: (600, 450)\n",
            "ISIC_0030586.jpg: (600, 450)\n",
            "ISIC_0027550.jpg: (600, 450)\n",
            "ISIC_0028499.jpg: (600, 450)\n",
            "ISIC_0027588.jpg: (600, 450)\n",
            "ISIC_0026138.jpg: (600, 450)\n",
            "ISIC_0025637.jpg: (600, 450)\n",
            "ISIC_0030036.jpg: (600, 450)\n",
            "ISIC_0027678.jpg: (600, 450)\n",
            "ISIC_0031823.jpg: (600, 450)\n",
            "ISIC_0025780.jpg: (600, 450)\n",
            "ISIC_0032349.jpg: (600, 450)\n",
            "ISIC_0025427.jpg: (600, 450)\n",
            "ISIC_0030341.jpg: (600, 450)\n",
            "ISIC_0032135.jpg: (600, 450)\n",
            "ISIC_0025831.jpg: (600, 450)\n",
            "ISIC_0026720.jpg: (600, 450)\n",
            "ISIC_0026549.jpg: (600, 450)\n",
            "ISIC_0030730.jpg: (600, 450)\n",
            "ISIC_0029210.jpg: (600, 450)\n",
            "ISIC_0027294.jpg: (600, 450)\n",
            "ISIC_0028558.jpg: (600, 450)\n",
            "ISIC_0029830.jpg: (600, 450)\n",
            "ISIC_0029417.jpg: (600, 450)\n",
            "ISIC_0033866.jpg: (600, 450)\n",
            "ISIC_0024418.jpg: (600, 450)\n",
            "ISIC_0032199.jpg: (600, 450)\n",
            "ISIC_0033358.jpg: (600, 450)\n",
            "ISIC_0031430.jpg: (600, 450)\n",
            "ISIC_0027668.jpg: (600, 450)\n",
            "ISIC_0029811.jpg: (600, 450)\n",
            "ISIC_0033151.jpg: (600, 450)\n",
            "ISIC_0026709.jpg: (600, 450)\n",
            "ISIC_0029610.jpg: (600, 450)\n",
            "ISIC_0031506.jpg: (600, 450)\n",
            "ISIC_0031659.jpg: (600, 450)\n",
            "ISIC_0024470.jpg: (600, 450)\n",
            "ISIC_0025605.jpg: (600, 450)\n",
            "ISIC_0030142.jpg: (600, 450)\n",
            "ISIC_0026872.jpg: (600, 450)\n",
            "ISIC_0025358.jpg: (600, 450)\n",
            "ISIC_0029573.jpg: (600, 450)\n",
            "ISIC_0024763.jpg: (600, 450)\n",
            "ISIC_0030408.jpg: (600, 450)\n",
            "ISIC_0030826.jpg: (600, 450)\n",
            "ISIC_0030794.jpg: (600, 450)\n",
            "ISIC_0026132.jpg: (600, 450)\n",
            "ISIC_0032947.jpg: (600, 450)\n",
            "ISIC_0029582.jpg: (600, 450)\n",
            "ISIC_0025803.jpg: (600, 450)\n",
            "ISIC_0030991.jpg: (600, 450)\n",
            "ISIC_0026100.jpg: (600, 450)\n",
            "ISIC_0030827.jpg: (600, 450)\n",
            "ISIC_0026525.jpg: (600, 450)\n",
            "ISIC_0029638.jpg: (600, 450)\n",
            "ISIC_0027178.jpg: (600, 450)\n",
            "ISIC_0025196.jpg: (600, 450)\n",
            "ISIC_0027700.jpg: (600, 450)\n",
            "ISIC_0026206.jpg: (600, 450)\n",
            "ISIC_0024329.jpg: (600, 450)\n",
            "ISIC_0025411.jpg: (600, 450)\n",
            "ISIC_0026625.jpg: (600, 450)\n",
            "ISIC_0025089.jpg: (600, 450)\n",
            "ISIC_0024579.jpg: (600, 450)\n",
            "ISIC_0025948.jpg: (600, 450)\n",
            "ISIC_0028076.jpg: (600, 450)\n",
            "ISIC_0025368.jpg: (600, 450)\n",
            "ISIC_0026203.jpg: (600, 450)\n",
            "ISIC_0028231.jpg: (600, 450)\n",
            "ISIC_0029932.jpg: (600, 450)\n",
            "ISIC_0032404.jpg: (600, 450)\n",
            "ISIC_0029598.jpg: (600, 450)\n",
            "ISIC_0025331.jpg: (600, 450)\n",
            "ISIC_0031874.jpg: (600, 450)\n",
            "ISIC_0032238.jpg: (600, 450)\n",
            "ISIC_0026905.jpg: (600, 450)\n",
            "ISIC_0027708.jpg: (600, 450)\n",
            "ISIC_0026645.jpg: (600, 450)\n",
            "ISIC_0029362.jpg: (600, 450)\n",
            "ISIC_0031743.jpg: (600, 450)\n",
            "ISIC_0028619.jpg: (600, 450)\n",
            "ISIC_0031211.jpg: (600, 450)\n",
            "ISIC_0030387.jpg: (600, 450)\n",
            "ISIC_0028659.jpg: (600, 450)\n",
            "ISIC_0025130.jpg: (600, 450)\n",
            "ISIC_0029541.jpg: (600, 450)\n",
            "ISIC_0024948.jpg: (600, 450)\n",
            "ISIC_0030825.jpg: (600, 450)\n",
            "ISIC_0029851.jpg: (600, 450)\n",
            "ISIC_0029567.jpg: (600, 450)\n",
            "ISIC_0026984.jpg: (600, 450)\n",
            "ISIC_0031929.jpg: (600, 450)\n",
            "ISIC_0032154.jpg: (600, 450)\n",
            "ISIC_0025712.jpg: (600, 450)\n",
            "ISIC_0024925.jpg: (600, 450)\n",
            "ISIC_0031431.jpg: (600, 450)\n",
            "ISIC_0033536.jpg: (600, 450)\n",
            "ISIC_0029025.jpg: (600, 450)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "# Paths\n",
        "metadata_csv = '/content/skin_cancer_dataset/HAM10000_metadata.csv'\n",
        "images_dir_1 = '/content/skin_cancer_dataset/HAM10000_images_part_1'\n",
        "images_dir_2 = '/content/skin_cancer_dataset/HAM10000_images_part_2'\n",
        "output_dir = 'resized_skin_lesions_256x256'  # Renamed for clarity\n",
        "\n",
        "# Read metadata\n",
        "df = pd.read_csv(metadata_csv)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Resize and pad to 256x256\n",
        "def resize_with_padding(img, size=(256, 256)):\n",
        "    return ImageOps.pad(img, size, method=Image.BICUBIC, color=(0, 0, 0), centering=(0.5, 0.5))\n",
        "\n",
        "# Helper: Locate image path\n",
        "def get_image_path(image_id):\n",
        "    fname = image_id + '.jpg'\n",
        "    path1 = os.path.join(images_dir_1, fname)\n",
        "    if os.path.exists(path1):\n",
        "        return path1\n",
        "    path2 = os.path.join(images_dir_2, fname)\n",
        "    if os.path.exists(path2):\n",
        "        return path2\n",
        "    return None\n",
        "\n",
        "# Organize and resize images\n",
        "not_found = []\n",
        "for idx, row in df.iterrows():\n",
        "    image_id = row['image_id']\n",
        "    class_label = row['dx']\n",
        "    src_img = get_image_path(image_id)\n",
        "\n",
        "    if src_img is None:\n",
        "        not_found.append(image_id)\n",
        "        continue\n",
        "\n",
        "    # Load and resize\n",
        "    img = Image.open(src_img).convert(\"RGB\")\n",
        "    img_resized = resize_with_padding(img)\n",
        "\n",
        "    # Save into class directory\n",
        "    class_dir = os.path.join(output_dir, class_label)\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    dst_img_path = os.path.join(class_dir, image_id + '.jpg')\n",
        "    img_resized.save(dst_img_path)\n",
        "\n",
        "print(\"✅ Resized and organized skin cancer images to 256x256!\")\n",
        "\n",
        "if not_found:\n",
        "    print(f\"⚠️ {len(not_found)} images were not found.\")\n",
        "\n",
        "# Optional: Directory structure preview\n",
        "def print_directory_tree(path, level=2):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        depth = root[len(path):].count(os.sep)\n",
        "        if depth > level:\n",
        "            continue\n",
        "        indent = ' ' * 4 * depth\n",
        "        print(f'{indent}{os.path.basename(root)}/')\n",
        "        if depth < level:\n",
        "            for f in files[:5]:\n",
        "                print(f'{indent}    {f}')\n",
        "\n",
        "print_directory_tree(output_dir, level=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfkFT-35k97w",
        "outputId": "625acd85-4cd6-4312-da23-3b5c72050893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Resized and organized skin cancer images to 256x256!\n",
            "resized_skin_lesions_256x256/\n",
            "    vasc/\n",
            "        ISIC_0025452.jpg\n",
            "        ISIC_0032557.jpg\n",
            "        ISIC_0025606.jpg\n",
            "        ISIC_0031197.jpg\n",
            "        ISIC_0032057.jpg\n",
            "    akiec/\n",
            "        ISIC_0026083.jpg\n",
            "        ISIC_0024575.jpg\n",
            "        ISIC_0026650.jpg\n",
            "        ISIC_0031672.jpg\n",
            "        ISIC_0030491.jpg\n",
            "    nv/\n",
            "        ISIC_0026841.jpg\n",
            "        ISIC_0030886.jpg\n",
            "        ISIC_0026550.jpg\n",
            "        ISIC_0024606.jpg\n",
            "        ISIC_0025962.jpg\n",
            "    df/\n",
            "        ISIC_0033860.jpg\n",
            "        ISIC_0027598.jpg\n",
            "        ISIC_0027008.jpg\n",
            "        ISIC_0027727.jpg\n",
            "        ISIC_0031429.jpg\n",
            "    bcc/\n",
            "        ISIC_0029083.jpg\n",
            "        ISIC_0024573.jpg\n",
            "        ISIC_0026687.jpg\n",
            "        ISIC_0028577.jpg\n",
            "        ISIC_0026528.jpg\n",
            "    mel/\n",
            "        ISIC_0032622.jpg\n",
            "        ISIC_0024961.jpg\n",
            "        ISIC_0033279.jpg\n",
            "        ISIC_0031529.jpg\n",
            "        ISIC_0027163.jpg\n",
            "    bkl/\n",
            "        ISIC_0026070.jpg\n",
            "        ISIC_0027780.jpg\n",
            "        ISIC_0026016.jpg\n",
            "        ISIC_0025856.jpg\n",
            "        ISIC_0029522.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink, display\n",
        "\n",
        "zip_file = 'synthetic_samples.zip'\n",
        "\n",
        "if os.path.exists(zip_file):\n",
        "    display(FileLink(zip_file))\n",
        "else:\n",
        "    print(f\"Zip file '{zip_file}' does not exist. Please generate synthetic images first.\")\n"
      ],
      "metadata": {
        "id": "H0-bk8OVvYt0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "353645d9-7512-499d-f397-3627f4d0a3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/content/synthetic_samples.zip"
            ],
            "text/html": [
              "<a href='synthetic_samples.zip' target='_blank'>synthetic_samples.zip</a><br>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "input_dir = 'synthetic_samples'\n",
        "output_filename = 'synthetic_samples'\n",
        "\n",
        "try:\n",
        "    if os.path.exists(input_dir) and os.path.isdir(input_dir):\n",
        "        shutil.make_archive(output_filename, 'zip', input_dir)\n",
        "        print(f\"✅ Directory '{input_dir}' has been zipped successfully into '{output_filename}.zip'\")\n",
        "    else:\n",
        "        print(f\"❌ Directory '{input_dir}' does not exist. Please check your path or generate images first.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npLZBTXSrbyq",
        "outputId": "5f1625d4-e96f-4f82-8d2d-2f939de96fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Directory 'synthetic_samples' has been zipped successfully into 'synthetic_samples.zip'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYHQlB02r6bp",
        "outputId": "59835c70-65b7-406a-946a-a9a076253056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "['.config', 'skin_cancer_dataset', 'projected_gan.py', 'synthetic_samples', 'generator.py', 'skin-cancer-mnist-ham10000.zip', '__pycache__', 'dataset.py', 'synthetic_samples.zip', 'differentiable_augmentation.py', 'generate_samples.py', 'efficient_net.py', 'organized_skin_lesions', 'checkpoints', 'efficientnet_lite1.pth', 'resized_skin_lesions_256x256', 'utils.py', 'kaggle.json', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('synthetic_samples.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "p1wVOEvgsmvX",
        "outputId": "6b0de2a6-c055-48e2-d4bd-f7b8754b0ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_373b7fa4-ed08-408e-bb39-c28d3cdf87a6\", \"synthetic_samples.zip\", 392770081)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('unzipped_data'))\n",
        "print(os.listdir('resized_images'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHCL-JLIyDUH",
        "outputId": "fa146396-f2ae-4ab8-ab2b-c1c506eb20ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['epoch_5', 'epoch_27', 'epoch_24', 'epoch_11', 'epoch_17', 'epoch_13', 'epoch_12', 'epoch_7', 'epoch_45', 'epoch_3', 'epoch_34', 'epoch_30', 'epoch_28', 'epoch_43', 'epoch_29', 'epoch_16', 'epoch_36', 'epoch_8', 'epoch_21', 'epoch_31', 'epoch_41', 'epoch_14', 'epoch_10', 'epoch_35', 'epoch_42', 'epoch_9', 'epoch_33', 'epoch_37', 'epoch_20', 'epoch_18', 'epoch_26', 'epoch_1', 'epoch_40', 'epoch_38', 'epoch_23', 'epoch_0', 'epoch_44', 'epoch_22', 'epoch_39', 'epoch_47', 'epoch_49', 'epoch_46', 'epoch_15', 'epoch_25', 'epoch_48', 'epoch_4', 'epoch_32', 'epoch_6', 'epoch_2', 'epoch_19']\n",
            "['img_61_label_0.png', 'img_9_label_0.png', 'img_31_label_0.png', 'img_13_label_0.png', 'img_58_label_0.png', 'img_91_label_0.png', 'img_55_label_0.png', 'img_97_label_0.png', 'img_64_label_0.png', 'img_52_label_0.png', 'img_43_label_0.png', 'img_82_label_0.png', 'img_99_label_0.png', 'img_94_label_0.png', 'img_24_label_0.png', 'img_17_label_0.png', 'img_33_label_0.png', 'img_96_label_0.png', 'img_7_label_0.png', 'img_27_label_0.png', 'img_28_label_0.png', 'img_35_label_0.png', 'img_47_label_0.png', 'img_14_label_0.png', 'img_34_label_0.png', 'img_70_label_0.png', 'img_93_label_0.png', 'img_88_label_0.png', 'img_77_label_0.png', 'img_85_label_0.png', 'img_18_label_0.png', 'img_69_label_0.png', 'img_20_label_0.png', 'img_54_label_0.png', 'img_51_label_0.png', 'img_0_label_0.png', 'img_26_label_0.png', 'img_22_label_0.png', 'img_60_label_0.png', 'img_2_label_0.png', 'img_74_label_0.png', 'img_38_label_0.png', 'img_16_label_0.png', 'img_95_label_0.png', 'img_44_label_0.png', 'img_56_label_0.png', 'img_59_label_0.png', 'img_62_label_0.png', 'img_25_label_0.png', 'img_42_label_0.png', 'img_5_label_0.png', 'img_53_label_0.png', 'img_32_label_0.png', 'img_66_label_0.png', 'img_41_label_0.png', 'img_1_label_0.png', 'img_86_label_0.png', 'img_76_label_0.png', 'img_15_label_0.png', 'img_81_label_0.png', 'img_36_label_0.png', 'img_23_label_0.png', 'img_48_label_0.png', 'img_71_label_0.png', 'img_92_label_0.png', 'img_30_label_0.png', 'img_78_label_0.png', 'img_10_label_0.png', 'img_89_label_0.png', 'img_72_label_0.png', 'img_12_label_0.png', 'img_11_label_0.png', 'img_63_label_0.png', 'img_49_label_0.png', 'img_19_label_0.png', 'img_73_label_0.png', 'img_75_label_0.png', 'img_39_label_0.png', 'img_6_label_0.png', 'img_57_label_0.png', 'img_68_label_0.png', 'img_87_label_0.png', 'img_8_label_0.png', 'img_21_label_0.png', 'img_4_label_0.png', 'img_80_label_0.png', 'img_37_label_0.png', 'img_83_label_0.png', 'img_45_label_0.png', 'img_46_label_0.png', 'img_90_label_0.png', 'img_98_label_0.png', 'img_65_label_0.png', 'img_3_label_0.png', 'img_84_label_0.png', 'img_50_label_0.png', 'img_79_label_0.png', 'img_40_label_0.png', 'img_29_label_0.png', 'img_67_label_0.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def calculate_uaci(img1, img2):\n",
        "    diff = np.abs(img1.astype(np.float32) - img2.astype(np.float32))\n",
        "    return np.mean(diff / 255.0) * 100\n",
        "\n",
        "# Set your folders\n",
        "real_dir = 'resized_images/'\n",
        "synthetic_dir = 'unzipped_data/epoch_49/'\n",
        "\n",
        "# Get sorted lists of image paths\n",
        "real_images = sorted(glob.glob(os.path.join(real_dir, '*.jpg')))\n",
        "synthetic_images = sorted(glob.glob(os.path.join(synthetic_dir, '*.png')))\n",
        "\n",
        "# Use the minimum number of images\n",
        "num_pairs = min(len(real_images), len(synthetic_images))\n",
        "real_images = real_images[:num_pairs]\n",
        "synthetic_images = synthetic_images[:num_pairs]\n",
        "\n",
        "ssim_scores, psnr_scores, uaci_scores = [], [], []\n",
        "\n",
        "for real_path, synth_path in zip(real_images, synthetic_images):\n",
        "    img1 = cv2.imread(real_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img2 = cv2.imread(synth_path, cv2.IMREAD_GRAYSCALE)\n",
        "    ssim_scores.append(ssim(img1, img2))\n",
        "    psnr_scores.append(psnr(img1, img2, data_range=img1.max() - img1.min()))\n",
        "    uaci_scores.append(calculate_uaci(img1, img2))\n",
        "\n",
        "print(f\"Average SSIM: {np.mean(ssim_scores):.4f}\")\n",
        "print(f\"Average PSNR: {np.mean(psnr_scores):.2f} dB\")\n",
        "print(f\"Average UACI: {np.mean(uaci_scores):.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smBTgjSSySkc",
        "outputId": "34d31e57-a1f9-495f-8ca2-12e451a18915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average SSIM: nan\n",
            "Average PSNR: nan dB\n",
            "Average UACI: nan%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive('checkpoints', 'zip', 'checkpoints')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6scWucSB4Ydc",
        "outputId": "bd9b1ba3-9556-4dd1-e649-e48779225f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/checkpoints.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkbtZ0t6BGkV",
        "outputId": "b4e46c26-cd4a-430e-d804-f28d2bf3b075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp checkpoints.zip /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "M5OCIv6vBQpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"synthetic_samples.zip\"  # e.g., \"organized_skin_lesions.zip\"\n",
        "extract_dir = \"unzipped_data\"  # Change if you want a different folder name\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n"
      ],
      "metadata": {
        "id": "wZ47ze_AVUM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DFFlY0uzaj_i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}